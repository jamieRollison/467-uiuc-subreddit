{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
        "\n",
        "This is an example of applying :class:`~sklearn.decomposition.NMF` and\n",
        ":class:`~sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
        "of documents and extract additive models of the topic structure of the\n",
        "corpus.  The output is a plot of topics, each represented as bar plot\n",
        "using top few words based on weights.\n",
        "\n",
        "Non-negative Matrix Factorization is applied with two different objective\n",
        "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\n",
        "The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
        "\n",
        "The default parameters (n_samples / n_features / n_components) should make\n",
        "the example runnable in a couple of tens of seconds. You can try to\n",
        "increase the dimensions of the problem, but be aware that the time\n",
        "complexity is polynomial in NMF. In LDA, the time complexity is\n",
        "proportional to (n_samples * iterations).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
        "#         Lars Buitinck\n",
        "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
        "# License: BSD 3 clause\n",
        "\n",
        "from time import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20\n",
        "batch_size = 128\n",
        "init = \"nndsvda\"\n",
        "\n",
        "\n",
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[-n_top_words:]\n",
        "        top_features = feature_names[top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
        "        for i in \"top right left\".split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('./uiuc_top_all.csv')\n",
        "# data_controversial = pd.read_csv('./uiuc_controversial_all.csv')\n",
        "# data = pd.concat([data, data_controversial])\n",
        "\n",
        "# replace NaN with empty string or 0 for numerical columns\n",
        "data['PostText'] = data['PostText'].replace(np.nan, '', regex=True)\n",
        "data['Title'] = data['Title'].replace(np.nan, '', regex=True)\n",
        "data['Sentiment'] = data['Sentiment'].replace(np.nan, 0, regex=True)\n",
        "\n",
        "years = [str(i) for i in range(2008, 2024)]\n",
        "\n",
        "data_by_year = {year: data[data['Date'].str.contains(year)] for year in years}\n",
        "\n",
        "# remove years with less than 2 posts\n",
        "data_by_year = {year: data_by_year[year] for year in years if len(data_by_year[year]) > 10}\n",
        "years = list(data_by_year.keys())\n",
        "\n",
        "print(years)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "year_model = {}\n",
        "for year in years:\n",
        "    df = data_by_year[year]\n",
        "\n",
        "    # concat post titles to body text (data)\n",
        "    post_titles = df['Title'].to_list()\n",
        "    post_body = df['PostText'].to_list()\n",
        "\n",
        "    dataset = [str(title + ' ' + body) for title, body in zip(post_titles, post_body)]\n",
        "\n",
        "    data_samples = dataset[:n_samples]\n",
        "    # print([type(d) for d in data_samples])\n",
        "\n",
        "\n",
        "    # Use tf-idf features for NMF.\n",
        "    print(\"Extracting tf-idf features for NMF...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        "    )\n",
        "    t0 = time()\n",
        "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "    print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "    # Fit the NMF model\n",
        "    print(\n",
        "        \"\\n\" * 2,\n",
        "        \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "        \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "        % (n_samples, n_features),\n",
        "    )\n",
        "    t0 = time()\n",
        "    nmf = NMF(\n",
        "        n_components=n_components,\n",
        "        random_state=1,\n",
        "        init=init,\n",
        "        beta_loss=\"kullback-leibler\",\n",
        "        solver=\"mu\",\n",
        "        max_iter=1000,\n",
        "        alpha_W=0.00005,\n",
        "        alpha_H=0.00005,\n",
        "        l1_ratio=0.5,\n",
        "    ).fit(tfidf)\n",
        "    print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    # plot_top_words(\n",
        "    #     nmf,\n",
        "    #     tfidf_feature_names,\n",
        "    #     n_top_words,\n",
        "    #     \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
        "    # )\n",
        "    year_model[year] = (nmf, tfidf_feature_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Object of type zip is not JSON serializable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[99], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m   json\u001b[38;5;241m.\u001b[39mdump(normalized_topics, f)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposts_by_topic.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 44\u001b[0m   \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposts_by_topic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type zip is not JSON serializable"
          ]
        }
      ],
      "source": [
        "import json\n",
        "topics = []\n",
        "posts_by_topic = {}\n",
        "for year, (nmf, tfidf_feature_names) in year_model.items():\n",
        "    # print(f\"Year: {year}\")\n",
        "    for topic_idx, topic in enumerate(nmf.components_):\n",
        "        top_features_ind = topic.argsort()[-3:]\n",
        "        top_features = tfidf_feature_names[top_features_ind]\n",
        "        # print(f\"Topic {topic_idx + 1}: {', '.join(top_features)}\")\n",
        "\n",
        "        # Get the posts in the year's data that include the top features\n",
        "        posts_with_top_features = data_by_year[year][data_by_year[year]['PostText'].str.contains('|'.join(top_features))]\n",
        "\n",
        "        posts_by_topic[\", \".join(top_features)] = [\n",
        "           { \"title\": title, \"url\": url } \n",
        "           for title, url in zip(posts_with_top_features['Title'], posts_with_top_features['URL'])]\n",
        "\n",
        "        proportion_posts = len(posts_with_top_features)\n",
        "\n",
        "        # Calculate the sentiment score for each post and sum them\n",
        "        sentiment_score = sum(posts_with_top_features['Sentiment'])\n",
        "\n",
        "        # print(f\"Sentiment Score: {sentiment_score}\")\n",
        "\n",
        "        topics.append(\n",
        "            {\n",
        "                \"year\": int(year),\n",
        "                \"words\": \", \".join(top_features),\n",
        "                \"sentiment\": sentiment_score,\n",
        "                \"count\": proportion_posts,\n",
        "            }\n",
        "        )\n",
        "\n",
        "normalized_topics = []\n",
        "for year in years:\n",
        "    year_topics = [t for t in topics if t['year'] == int(year)]\n",
        "    ranking = np.argsort([t['count'] for t in year_topics])\n",
        "    for i, t in enumerate(year_topics):\n",
        "        t['rank'] = int(ranking[i])\n",
        "        normalized_topics.append(t)\n",
        "\n",
        "with open('topics.json', 'w') as f:\n",
        "  json.dump(normalized_topics, f)\n",
        "\n",
        "with open('posts_by_topic.json', 'w') as f:\n",
        "  json.dump(posts_by_topic, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
        "# # to filter out useless terms early on: the posts are stripped of headers,\n",
        "# # footers and quoted replies, and common English words, words occurring in\n",
        "# # only one document or in at least 95% of the documents are removed.\n",
        "\n",
        "# print(\"Loading dataset...\")\n",
        "# t0 = time()\n",
        "# data, _ = fetch_20newsgroups(\n",
        "#     shuffle=True,\n",
        "#     random_state=1,\n",
        "#     remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "#     return_X_y=True,\n",
        "# )\n",
        "# data_samples = data[:n_samples]\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Use tf-idf features for NMF.\n",
        "# print(\"Extracting tf-idf features for NMF...\")\n",
        "# tfidf_vectorizer = TfidfVectorizer(\n",
        "#     max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        "# )\n",
        "# t0 = time()\n",
        "# tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# # Use tf (raw term count) features for LDA.\n",
        "# print(\"Extracting tf features for LDA...\")\n",
        "# tf_vectorizer = CountVectorizer(\n",
        "#     max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        "# )\n",
        "# t0 = time()\n",
        "# tf = tf_vectorizer.fit_transform(data_samples)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "# print()\n",
        "\n",
        "# # Fit the NMF model\n",
        "# print(\n",
        "#     \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "#     \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\n",
        "# )\n",
        "# t0 = time()\n",
        "# nmf = NMF(\n",
        "#     n_components=n_components,\n",
        "#     random_state=1,\n",
        "#     init=init,\n",
        "#     beta_loss=\"frobenius\",\n",
        "#     alpha_W=0.00005,\n",
        "#     alpha_H=0.00005,\n",
        "#     l1_ratio=1,\n",
        "# ).fit(tfidf)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "\n",
        "# tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# plot_top_words(\n",
        "#     nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
        "# )\n",
        "\n",
        "# # Fit the NMF model\n",
        "# print(\n",
        "#     \"\\n\" * 2,\n",
        "#     \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "#     \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "#     % (n_samples, n_features),\n",
        "# )\n",
        "# t0 = time()\n",
        "# nmf = NMF(\n",
        "#     n_components=n_components,\n",
        "#     random_state=1,\n",
        "#     init=init,\n",
        "#     beta_loss=\"kullback-leibler\",\n",
        "#     solver=\"mu\",\n",
        "#     max_iter=1000,\n",
        "#     alpha_W=0.00005,\n",
        "#     alpha_H=0.00005,\n",
        "#     l1_ratio=0.5,\n",
        "# ).fit(tfidf)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# plot_top_words(\n",
        "#     nmf,\n",
        "#     tfidf_feature_names,\n",
        "#     n_top_words,\n",
        "#     \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
        "# )\n",
        "\n",
        "# # Fit the MiniBatchNMF model\n",
        "# print(\n",
        "#     \"\\n\" * 2,\n",
        "#     \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf \"\n",
        "#     \"features, n_samples=%d and n_features=%d, batch_size=%d...\"\n",
        "#     % (n_samples, n_features, batch_size),\n",
        "# )\n",
        "# t0 = time()\n",
        "# mbnmf = MiniBatchNMF(\n",
        "#     n_components=n_components,\n",
        "#     random_state=1,\n",
        "#     batch_size=batch_size,\n",
        "#     init=init,\n",
        "#     beta_loss=\"frobenius\",\n",
        "#     alpha_W=0.00005,\n",
        "#     alpha_H=0.00005,\n",
        "#     l1_ratio=0.5,\n",
        "# ).fit(tfidf)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "\n",
        "# tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# plot_top_words(\n",
        "#     mbnmf,\n",
        "#     tfidf_feature_names,\n",
        "#     n_top_words,\n",
        "#     \"Topics in MiniBatchNMF model (Frobenius norm)\",\n",
        "# )\n",
        "\n",
        "# # Fit the MiniBatchNMF model\n",
        "# print(\n",
        "#     \"\\n\" * 2,\n",
        "#     \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler \"\n",
        "#     \"divergence) with tf-idf features, n_samples=%d and n_features=%d, \"\n",
        "#     \"batch_size=%d...\" % (n_samples, n_features, batch_size),\n",
        "# )\n",
        "# t0 = time()\n",
        "# mbnmf = MiniBatchNMF(\n",
        "#     n_components=n_components,\n",
        "#     random_state=1,\n",
        "#     batch_size=batch_size,\n",
        "#     init=init,\n",
        "#     beta_loss=\"kullback-leibler\",\n",
        "#     alpha_W=0.00005,\n",
        "#     alpha_H=0.00005,\n",
        "#     l1_ratio=0.5,\n",
        "# ).fit(tfidf)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# plot_top_words(\n",
        "#     mbnmf,\n",
        "#     tfidf_feature_names,\n",
        "#     n_top_words,\n",
        "#     \"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\n",
        "# )\n",
        "\n",
        "# print(\n",
        "#     \"\\n\" * 2,\n",
        "#     \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
        "#     % (n_samples, n_features),\n",
        "# )\n",
        "# lda = LatentDirichletAllocation(\n",
        "#     n_components=n_components,\n",
        "#     max_iter=5,\n",
        "#     learning_method=\"online\",\n",
        "#     learning_offset=50.0,\n",
        "#     random_state=0,\n",
        "# )\n",
        "# t0 = time()\n",
        "# lda.fit(tf)\n",
        "# print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "# plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#ff004c', '#ff024a', '#ff044a', '#ff0648', '#ff0849', '#ff0a47', '#ff0c47', '#ff0e46', '#ff1046', '#ff1145', '#ff1345', '#ff1443', '#ff1644', '#ff1745', '#ff1a43', '#ff1b44', '#ff1d43', '#ff1e43', '#ff2143', '#ff2243', '#ff2443', '#ff2644', '#ff2844', '#ff2b45', '#ff2c44', '#ff2e45', '#ff3045', '#ff3245', '#ff3546', '#ff3747', '#ff3947', '#ff3b48', '#ff3f49', '#ff414a', '#ff434b', '#ff474c', '#ff494d', '#ff4c4d', '#ff4e4f', '#ff5150', '#ff5351', '#ff5753', '#ff5a53', '#ff5c54', '#ff5f56', '#ff6257', '#ff6559', '#ff665a', '#ff6a5b', '#ff6d5d', '#ff6f5d', '#ff725f', '#ff7461', '#ff7763', '#ff7a63', '#ff7d65', '#fd7e66', '#fb7f67', '#fa8168', '#f88269', '#f6846a', '#f4856b', '#f3866c', '#f1876d', '#f0896e', '#ee896e', '#ec8b6f', '#ea8c70', '#e98d71', '#e78e71', '#e48e72', '#e38f73', '#e18f74', '#e09174', '#dc9075', '#db9176', '#d99276', '#d79277', '#d59278', '#d29278', '#d19379', '#ce9279', '#cc927a', '#c9927a', '#c7927b', '#c5927b', '#c3927b', '#c0917c', '#be917c', '#bc917c', '#ba907c', '#b7907c', '#b58f7c', '#b28e7c', '#b08e7c', '#ae8c7c', '#ac8c7c', '#a98b7c', '#a78b7c', '#a58a7c', '#a2887b', '#a0887b', '#9d877b', '#9b867a', '#99857a', '#97847a', '#958379', '#918178', '#8f8078', '#8d7f77', '#8b7e77', '#897d76', '#887d76', '#867b75', '#837a74', '#817873', '#7f7772', '#7d7671', '#7b7471', '#797370', '#77726f', '#75716e', '#74706d', '#726f6c', '#6f6c6b', '#6e6c6a', '#6c6a69', '#6b6969', '#696867', '#676766', '#666767', '#676868', '#68696a', '#696a6b', '#696b6c', '#6a6d6e', '#6b6e6f', '#6c6f71', '#6c7072', '#6d7173', '#6e7375', '#6f7376', '#6f7578', '#70767a', '#71787c', '#71797d', '#727a7e', '#737b80', '#737c81', '#747e83', '#747f84', '#758086', '#758188', '#768389', '#76848b', '#77858c', '#77878e', '#788890', '#798a93', '#798b94', '#798c95', '#7a8d98', '#7a8e99', '#7a919b', '#7b929e', '#7b93a0', '#7b95a1', '#7c96a4', '#7c98a6', '#7c99a7', '#7c9baa', '#7c9dad', '#7c9eaf', '#7c9fb1', '#7ca0b3', '#7ca2b6', '#7ca4b9', '#7ca6bc', '#7ca7bd', '#7ca8c0', '#7babc3', '#7bacc5', '#7badc8', '#7aafca', '#7ab0cd', '#79b1d0', '#78b2d3', '#77b4d6', '#76b6d9', '#76b7dc', '#75b8de', '#74b9e1', '#73bae4', '#71bbe7', '#71bde9', '#6fbeec', '#6ebfef', '#6cc0f2', '#6bc0f4', '#6ac1f7', '#68c1f9', '#67c2fd', '#65c3ff', '#63c2ff', '#61c0ff', '#5ebeff', '#5cbdff', '#59bbff', '#57b9ff', '#55b9ff', '#53b7ff', '#52b6ff', '#50b4ff', '#4db2ff', '#4bb0ff', '#4ab0ff', '#48afff', '#47adff', '#45abff', '#43a9ff', '#41a9ff', '#40a8ff', '#3fa6ff', '#3da5ff', '#3ca3ff', '#3ba2ff', '#39a0ff', '#389fff', '#379eff', '#369dff', '#339dff', '#329cff', '#329aff', '#3199ff', '#3098ff', '#2d96ff', '#2c96ff', '#2b95ff', '#2a94ff', '#2993ff', '#2890ff', '#278fff', '#248eff', '#238eff', '#218cff', '#208bff', '#1f8aff', '#1d8aff', '#1c89ff', '#1a87ff', '#1686ff', '#1583ff', '#1381ff', '#1180ff', '#0f7fff', '#0d7eff']\n"
          ]
        }
      ],
      "source": [
        "import colorsys\n",
        "\n",
        "def adjust_colors(colors, saturation_diff):\n",
        "    adjusted_colors = []\n",
        "    for color in colors:\n",
        "        # Convert hex color to RGB\n",
        "        rgb = tuple(int(color[i:i+2], 16) for i in (1, 3, 5))\n",
        "\n",
        "        # Convert RGB to HSV\n",
        "        hsv = colorsys.rgb_to_hsv(rgb[0]/255, rgb[1]/255, rgb[2]/255)\n",
        "\n",
        "        # Adjust luminance\n",
        "        adjusted_saturation = max(0, min(1, hsv[1] + saturation_diff))\n",
        "\n",
        "        # Convert HSV back to RGB\n",
        "        adjusted_rgb = colorsys.hsv_to_rgb(hsv[0], hsv[1], adjusted_saturation)\n",
        "\n",
        "        # Convert RGB to hex\n",
        "        adjusted_hex = '#%02x%02x%02x' % tuple(int(c * 255) for c in adjusted_rgb)\n",
        "\n",
        "        adjusted_colors.append(adjusted_hex)\n",
        "\n",
        "    return adjusted_colors\n",
        "\n",
        "# Example usage\n",
        "colors = [\n",
        "    \"#67001f\",\n",
        "    \"#6a011f\",\n",
        "    \"#6d0220\",\n",
        "    \"#700320\",\n",
        "    \"#730421\",\n",
        "    \"#760521\",\n",
        "    \"#790622\",\n",
        "    \"#7b0722\",\n",
        "    \"#7e0823\",\n",
        "    \"#810923\",\n",
        "    \"#840a24\",\n",
        "    \"#870b24\",\n",
        "    \"#8a0c25\",\n",
        "    \"#8c0d26\",\n",
        "    \"#8f0f26\",\n",
        "    \"#921027\",\n",
        "    \"#941127\",\n",
        "    \"#971228\",\n",
        "    \"#9a1429\",\n",
        "    \"#9c1529\",\n",
        "    \"#9f172a\",\n",
        "    \"#a1182b\",\n",
        "    \"#a41a2c\",\n",
        "    \"#a61c2d\",\n",
        "    \"#a81d2d\",\n",
        "    \"#aa1f2e\",\n",
        "    \"#ad212f\",\n",
        "    \"#af2330\",\n",
        "    \"#b12531\",\n",
        "    \"#b32732\",\n",
        "    \"#b52933\",\n",
        "    \"#b72b34\",\n",
        "    \"#b82e35\",\n",
        "    \"#ba3036\",\n",
        "    \"#bc3238\",\n",
        "    \"#be3539\",\n",
        "    \"#bf373a\",\n",
        "    \"#c13a3b\",\n",
        "    \"#c33c3d\",\n",
        "    \"#c43f3e\",\n",
        "    \"#c6413f\",\n",
        "    \"#c74441\",\n",
        "    \"#c94742\",\n",
        "    \"#ca4943\",\n",
        "    \"#cc4c45\",\n",
        "    \"#cd4f46\",\n",
        "    \"#ce5248\",\n",
        "    \"#d0544a\",\n",
        "    \"#d1574b\",\n",
        "    \"#d25a4d\",\n",
        "    \"#d45d4e\",\n",
        "    \"#d56050\",\n",
        "    \"#d66252\",\n",
        "    \"#d86554\",\n",
        "    \"#d96855\",\n",
        "    \"#da6b57\",\n",
        "    \"#db6d59\",\n",
        "    \"#dd705b\",\n",
        "    \"#de735d\",\n",
        "    \"#df755f\",\n",
        "    \"#e07861\",\n",
        "    \"#e17b63\",\n",
        "    \"#e27d65\",\n",
        "    \"#e48067\",\n",
        "    \"#e58369\",\n",
        "    \"#e6856b\",\n",
        "    \"#e7886d\",\n",
        "    \"#e88b6f\",\n",
        "    \"#e98d71\",\n",
        "    \"#ea9073\",\n",
        "    \"#eb9276\",\n",
        "    \"#ec9578\",\n",
        "    \"#ed977a\",\n",
        "    \"#ee9a7c\",\n",
        "    \"#ee9c7f\",\n",
        "    \"#ef9f81\",\n",
        "    \"#f0a183\",\n",
        "    \"#f1a486\",\n",
        "    \"#f2a688\",\n",
        "    \"#f2a88b\",\n",
        "    \"#f3ab8d\",\n",
        "    \"#f4ad90\",\n",
        "    \"#f4af92\",\n",
        "    \"#f5b295\",\n",
        "    \"#f5b497\",\n",
        "    \"#f6b69a\",\n",
        "    \"#f6b89c\",\n",
        "    \"#f7ba9f\",\n",
        "    \"#f7bda1\",\n",
        "    \"#f8bfa4\",\n",
        "    \"#f8c1a6\",\n",
        "    \"#f8c3a9\",\n",
        "    \"#f9c5ab\",\n",
        "    \"#f9c7ae\",\n",
        "    \"#f9c9b0\",\n",
        "    \"#facab3\",\n",
        "    \"#faccb5\",\n",
        "    \"#faceb8\",\n",
        "    \"#fad0ba\",\n",
        "    \"#fad2bc\",\n",
        "    \"#fad3bf\",\n",
        "    \"#fad5c1\",\n",
        "    \"#fbd7c4\",\n",
        "    \"#fbd8c6\",\n",
        "    \"#fbdac8\",\n",
        "    \"#fbdbca\",\n",
        "    \"#fbddcc\",\n",
        "    \"#fadecf\",\n",
        "    \"#fae0d1\",\n",
        "    \"#fae1d3\",\n",
        "    \"#fae2d5\",\n",
        "    \"#fae3d7\",\n",
        "    \"#fae5d8\",\n",
        "    \"#fae6da\",\n",
        "    \"#f9e7dc\",\n",
        "    \"#f9e8de\",\n",
        "    \"#f9e9e0\",\n",
        "    \"#f8eae1\",\n",
        "    \"#f8eae3\",\n",
        "    \"#f7ebe4\",\n",
        "    \"#f7ece6\",\n",
        "    \"#f6ede7\",\n",
        "    \"#f6ede8\",\n",
        "    \"#f5eee9\",\n",
        "    \"#f4eeeb\",\n",
        "    \"#f4efec\",\n",
        "    \"#f3efed\",\n",
        "    \"#f2efed\",\n",
        "    \"#f1efee\",\n",
        "    \"#f0f0ef\",\n",
        "    \"#eff0f0\",\n",
        "    \"#eef0f0\",\n",
        "    \"#edf0f1\",\n",
        "    \"#eceff1\",\n",
        "    \"#ebeff1\",\n",
        "    \"#eaeff2\",\n",
        "    \"#e9eff2\",\n",
        "    \"#e7eef2\",\n",
        "    \"#e6eef2\",\n",
        "    \"#e5edf2\",\n",
        "    \"#e3edf2\",\n",
        "    \"#e2ecf2\",\n",
        "    \"#e0ecf2\",\n",
        "    \"#dfebf2\",\n",
        "    \"#ddeaf2\",\n",
        "    \"#dbeaf1\",\n",
        "    \"#dae9f1\",\n",
        "    \"#d8e8f1\",\n",
        "    \"#d6e7f0\",\n",
        "    \"#d4e6f0\",\n",
        "    \"#d3e6f0\",\n",
        "    \"#d1e5ef\",\n",
        "    \"#cfe4ef\",\n",
        "    \"#cde3ee\",\n",
        "    \"#cbe2ee\",\n",
        "    \"#c9e1ed\",\n",
        "    \"#c7e0ed\",\n",
        "    \"#c5dfec\",\n",
        "    \"#c2ddec\",\n",
        "    \"#c0dceb\",\n",
        "    \"#bedbea\",\n",
        "    \"#bcdaea\",\n",
        "    \"#bad9e9\",\n",
        "    \"#b7d8e8\",\n",
        "    \"#b5d7e8\",\n",
        "    \"#b2d5e7\",\n",
        "    \"#b0d4e6\",\n",
        "    \"#aed3e6\",\n",
        "    \"#abd1e5\",\n",
        "    \"#a9d0e4\",\n",
        "    \"#a6cfe3\",\n",
        "    \"#a3cde3\",\n",
        "    \"#a1cce2\",\n",
        "    \"#9ecae1\",\n",
        "    \"#9cc9e0\",\n",
        "    \"#99c7e0\",\n",
        "    \"#96c6df\",\n",
        "    \"#93c4de\",\n",
        "    \"#91c3dd\",\n",
        "    \"#8ec1dc\",\n",
        "    \"#8bc0db\",\n",
        "    \"#88beda\",\n",
        "    \"#85bcd9\",\n",
        "    \"#83bbd8\",\n",
        "    \"#80b9d7\",\n",
        "    \"#7db7d7\",\n",
        "    \"#7ab5d6\",\n",
        "    \"#77b3d5\",\n",
        "    \"#74b2d4\",\n",
        "    \"#71b0d3\",\n",
        "    \"#6faed2\",\n",
        "    \"#6cacd1\",\n",
        "    \"#69aad0\",\n",
        "    \"#66a8cf\",\n",
        "    \"#64a7ce\",\n",
        "    \"#61a5cd\",\n",
        "    \"#5ea3cc\",\n",
        "    \"#5ba1cb\",\n",
        "    \"#599fca\",\n",
        "    \"#569dc9\",\n",
        "    \"#549bc8\",\n",
        "    \"#5199c7\",\n",
        "    \"#4f98c6\",\n",
        "    \"#4d96c5\",\n",
        "    \"#4b94c4\",\n",
        "    \"#4892c3\",\n",
        "    \"#4690c2\",\n",
        "    \"#448ec1\",\n",
        "    \"#428cc0\",\n",
        "    \"#408bbf\",\n",
        "    \"#3e89be\",\n",
        "    \"#3d87bd\",\n",
        "    \"#3b85bc\",\n",
        "    \"#3983bb\",\n",
        "    \"#3781ba\",\n",
        "    \"#3680b9\",\n",
        "    \"#347eb7\",\n",
        "    \"#337cb6\",\n",
        "    \"#317ab5\",\n",
        "    \"#3078b4\",\n",
        "    \"#2e76b2\",\n",
        "    \"#2d75b1\",\n",
        "    \"#2c73b0\",\n",
        "    \"#2a71ae\",\n",
        "    \"#296fad\",\n",
        "    \"#286dab\",\n",
        "    \"#266baa\",\n",
        "    \"#2569a8\",\n",
        "    \"#2467a6\",\n",
        "    \"#2365a4\",\n",
        "    \"#2164a2\",\n",
        "    \"#2062a0\",\n",
        "    \"#1f609e\",\n",
        "    \"#1e5e9c\",\n",
        "    \"#1d5c9a\",\n",
        "    \"#1b5a98\",\n",
        "    \"#1a5895\",\n",
        "    \"#195693\",\n",
        "    \"#185490\",\n",
        "    \"#17528e\",\n",
        "    \"#164f8b\",\n",
        "    \"#154d89\",\n",
        "    \"#134b86\",\n",
        "    \"#124983\",\n",
        "    \"#114781\",\n",
        "    \"#10457e\",\n",
        "    \"#0f437b\",\n",
        "    \"#0e4178\",\n",
        "    \"#0d3f75\",\n",
        "    \"#0c3d73\",\n",
        "    \"#0a3b70\",\n",
        "    \"#09386d\",\n",
        "    \"#08366a\",\n",
        "    \"#073467\",\n",
        "    \"#063264\",\n",
        "    \"#053061\",\n",
        "]\n",
        "satruationdiff = 0.1\n",
        "adjusted_colors = adjust_colors(colors, satruationdiff)\n",
        "print(adjusted_colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
